{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import OrderedDict as od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPREAD_URL = 'https://classic.sportsbookreview.com/betting-odds/ncaa-basketball/?date='     #date in the form of 20141114\n",
    "ML_URL = 'https://classic.sportsbookreview.com/betting-odds/ncaa-basketball/money-line/?date='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinnacle_id = '238'\n",
    "fiveDimes_id = '19'\n",
    "bookMaker_id = '93'\n",
    "betOnline_id = '1096'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_dates(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Formats dates into the format accepted by SBR\n",
    "    Input should be in the form 'MM/DD/YYYY'\n",
    "    \"\"\"\n",
    "    season = pd.date_range(start=start_date, end=end_date)\n",
    "    date_range = []\n",
    "    \n",
    "    for dates in season:\n",
    "        date = str(dates)\n",
    "        for_date = str(date[:4]) + str(date[5:7]) + str(date[8:10]) \n",
    "        date_range.append(for_date)\n",
    "    return date_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_date_format(date):\n",
    "    todays_date = pd.to_datetime(np.datetime64(date))\n",
    "    return todays_date.strftime('%m/%d/%Y')\n",
    "   \n",
    "def sbr_date_format(date):\n",
    "    desired_date = pd.to_datetime(np.datetime64(date))\n",
    "    print(desired_date)\n",
    "    return desired_date.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbb14 = season_dates('11/14/2014', '04/06/2015')\n",
    "cbb15 = season_dates('11/13/2015', '04/04/2016')\n",
    "cbb16 = season_dates('11/11/2016', '04/03/2017')\n",
    "cbb17 = season_dates('11/10/2017', '04/02/2018')\n",
    "cbb18 = season_dates('11/16/2018', '04/08/2019')\n",
    "cbb19 = season_dates('11/05/2019', '04/06/2020')\n",
    "\n",
    "date_list = [cbb14, cbb15, cbb16, cbb17, cbb18, cbb19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_db(date_list, URL=SPREAD_URL):\n",
    "    \"\"\"\n",
    "    Collects the beautiful soup data and sorts into and sorts in a dictionary\n",
    "    whose keys are the date ('YYYYMMDD') and values are the URLs\n",
    "    URL parameter allows for changing betweet the url for gathering spreads (SPREAD_URL, default)\n",
    "    and the url for gathering moneyline data(ML_URL)\n",
    "    \"\"\"\n",
    "    todays_date = pd.to_datetime(np.datetime64('today'))\n",
    "    date = todays_date.strftime('%Y%m%d')\n",
    "    if todays_date > pd.to_datetime(date_list[-1]):\n",
    "        pages = {'last_update': 'Complete', 'start_date' : date_list[0], 'end_date' : date_list[-1]}\n",
    "    else:\n",
    "        pages = {'last_update': todays_date, 'start_date' : date_list[0], 'end_date' : date_list[-1]}\n",
    "        date_range = date_list[0 : date_list.index(date) + 1]\n",
    "        \n",
    "    pages_db = {}\n",
    "\n",
    "    for page in date_range:\n",
    "        url = URL + str(page)\n",
    "        daily_page = requests.get(url)\n",
    "        pages_db[str(page)] = daily_page\n",
    "    if 'pages' not in pages:\n",
    "        pages['pages'] = pages_db\n",
    "    else:\n",
    "        pages['pages'].update(page_db)\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#money14 = page_db(cbb14, URL=ML_URL)\n",
    "#money15 = page_db(cbb15, URL=ML_URL)\n",
    "#money16 = page_db(cbb16, URL=ML_URL)\n",
    "#money17 = page_db(cbb17, URL=ML_URL)\n",
    "#money18 = page_db(cbb18, URL=ML_URL)\n",
    "#money19 = page_db(cbb19, URL=ML_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_update': Timestamp('2020-02-05 00:00:00'),\n",
       " 'start_date': '20191105',\n",
       " 'end_date': '20200406',\n",
       " 'pages': {'20191105': <Response [200]>,\n",
       "  '20191106': <Response [200]>,\n",
       "  '20191107': <Response [200]>,\n",
       "  '20191108': <Response [200]>,\n",
       "  '20191109': <Response [200]>,\n",
       "  '20191110': <Response [200]>,\n",
       "  '20191111': <Response [200]>,\n",
       "  '20191112': <Response [200]>,\n",
       "  '20191113': <Response [200]>,\n",
       "  '20191114': <Response [200]>,\n",
       "  '20191115': <Response [200]>,\n",
       "  '20191116': <Response [200]>,\n",
       "  '20191117': <Response [200]>,\n",
       "  '20191118': <Response [200]>,\n",
       "  '20191119': <Response [200]>,\n",
       "  '20191120': <Response [200]>,\n",
       "  '20191121': <Response [200]>,\n",
       "  '20191122': <Response [200]>,\n",
       "  '20191123': <Response [200]>,\n",
       "  '20191124': <Response [200]>,\n",
       "  '20191125': <Response [200]>,\n",
       "  '20191126': <Response [200]>,\n",
       "  '20191127': <Response [200]>,\n",
       "  '20191128': <Response [200]>,\n",
       "  '20191129': <Response [200]>,\n",
       "  '20191130': <Response [200]>,\n",
       "  '20191201': <Response [200]>,\n",
       "  '20191202': <Response [200]>,\n",
       "  '20191203': <Response [200]>,\n",
       "  '20191204': <Response [200]>,\n",
       "  '20191205': <Response [200]>,\n",
       "  '20191206': <Response [200]>,\n",
       "  '20191207': <Response [200]>,\n",
       "  '20191208': <Response [200]>,\n",
       "  '20191209': <Response [200]>,\n",
       "  '20191210': <Response [200]>,\n",
       "  '20191211': <Response [200]>,\n",
       "  '20191212': <Response [200]>,\n",
       "  '20191213': <Response [200]>,\n",
       "  '20191214': <Response [200]>,\n",
       "  '20191215': <Response [200]>,\n",
       "  '20191216': <Response [200]>,\n",
       "  '20191217': <Response [200]>,\n",
       "  '20191218': <Response [200]>,\n",
       "  '20191219': <Response [200]>,\n",
       "  '20191220': <Response [200]>,\n",
       "  '20191221': <Response [200]>,\n",
       "  '20191222': <Response [200]>,\n",
       "  '20191223': <Response [200]>,\n",
       "  '20191224': <Response [200]>,\n",
       "  '20191225': <Response [200]>,\n",
       "  '20191226': <Response [200]>,\n",
       "  '20191227': <Response [200]>,\n",
       "  '20191228': <Response [200]>,\n",
       "  '20191229': <Response [200]>,\n",
       "  '20191230': <Response [200]>,\n",
       "  '20191231': <Response [200]>,\n",
       "  '20200101': <Response [200]>,\n",
       "  '20200102': <Response [200]>,\n",
       "  '20200103': <Response [200]>,\n",
       "  '20200104': <Response [200]>,\n",
       "  '20200105': <Response [200]>,\n",
       "  '20200106': <Response [200]>,\n",
       "  '20200107': <Response [200]>,\n",
       "  '20200108': <Response [200]>,\n",
       "  '20200109': <Response [200]>,\n",
       "  '20200110': <Response [200]>,\n",
       "  '20200111': <Response [200]>,\n",
       "  '20200112': <Response [200]>,\n",
       "  '20200113': <Response [200]>,\n",
       "  '20200114': <Response [200]>,\n",
       "  '20200115': <Response [200]>,\n",
       "  '20200116': <Response [200]>,\n",
       "  '20200117': <Response [200]>,\n",
       "  '20200118': <Response [200]>,\n",
       "  '20200119': <Response [200]>,\n",
       "  '20200120': <Response [200]>,\n",
       "  '20200121': <Response [200]>,\n",
       "  '20200122': <Response [200]>,\n",
       "  '20200123': <Response [200]>,\n",
       "  '20200124': <Response [200]>,\n",
       "  '20200125': <Response [200]>,\n",
       "  '20200126': <Response [200]>,\n",
       "  '20200127': <Response [200]>,\n",
       "  '20200128': <Response [200]>,\n",
       "  '20200129': <Response [200]>,\n",
       "  '20200130': <Response [200]>,\n",
       "  '20200131': <Response [200]>,\n",
       "  '20200201': <Response [200]>,\n",
       "  '20200202': <Response [200]>,\n",
       "  '20200203': <Response [200]>,\n",
       "  '20200204': <Response [200]>,\n",
       "  '20200205': <Response [200]>}}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#page_db14 = page_db(cbb14)\n",
    "#page_db15 = page_db(cbb15)\n",
    "#page_db16 = page_db(cbb16)\n",
    "#page_db17 = page_db(cbb17)\n",
    "#page_db18 = page_db(cbb18)\n",
    "#page_db19 = page_db(cbb19)\n",
    "page_db19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_db(db, bet_type='spread'):\n",
    "    \"\"\"\n",
    "    Pickling dictionary of SBR page files with the naming convention\n",
    "    'NCAAB' followed by the year. (exp: 'NCCAB_2019.csv')\n",
    "    \"\"\"    \n",
    "    year = db['start_date'][0:4]\n",
    "\n",
    "    if bet_type == 'moneyline':\n",
    "        #csv_filename = \"NCAAB_ML\" + year + '.csv'\n",
    "        pickle_filename = \"NCAAB_ML\" + year + '.pickle'\n",
    "    else:\n",
    "        #csv_filename = \"NCAAB_\" + year + '.csv'\n",
    "        pickle_filename = \"NCAAB_\" + year + '.pickle'\n",
    "        \n",
    "    pickle_db = open(pickle_filename, 'wb')\n",
    "    pickle.dump(db, pickle_db)\n",
    "    pickle_db.close()\n",
    "    return pickle_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cbb_db_list = [page_db14, page_db15, page_db16, page_db17, page_db18, page_db19]\n",
    "#pickled_spreads = []\n",
    "#for file in cbb_db_list:\n",
    "#    pickled_spreads.append(pickle_db(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ml_db_list = [money14, money15, money16, money17, money18, money19]\n",
    "#pickled_mls = []\n",
    "#for file in ml_db_list:\n",
    "#    pickled_mls.append(pickle_db(file, bet_type='moneyline'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_spreads = ['NCAAB_2014.pickle', 'NCAAB_2015.pickle', 'NCAAB_2016.pickle', 'NCAAB_2017.pickle', 'NCAAB_2018.pickle', 'NCAAB_2019.pickle']\n",
    "pickled_mls = ['NCAAB_ML2014.pickle', 'NCAAB_ML2015.pickle', 'NCAAB_ML2016.pickle', 'NCAAB_ML2017.pickle', 'NCAAB_ML2018.pickle', 'NCAAB_ML2019.pickle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pickle(pickled_filenames, bet_type=None):\n",
    "    \"\"\"\n",
    "    Checks to see if databases need to be updated and opens files back in to memory.\n",
    "    \"\"\"\n",
    "    betType = bet_type\n",
    "    memory_db = []\n",
    "    for file in pickled_filenames:\n",
    "        try:\n",
    "            pickle_in = open(file, 'rb')\n",
    "            open_file = pickle.load(pickle_in)\n",
    "        except:\n",
    "            print('{} already open'.format(str(file)))\n",
    "\n",
    "        if open_file['last_update'] != 'Complete':\n",
    "            date_range = season_dates(open_file['last_update'],\n",
    "                                      season_date_format('today')) #creates date range from last update until current date\n",
    "            updated_page_db = page_db(date_range)                  #pulls missing pages from web, returns new dictionary \n",
    "            if open_file['last_update'] == open_file['end_date']:\n",
    "                  del open_file['pages'][open_file['last_update']]              \n",
    "            open_file['pages'].update(updated_page_db['pages'])\n",
    "            open_file['last_update'] = updated_page_db['last_update']\n",
    "            open_file = pickle_db(open_file, bet_type=betType)\n",
    "            print('Updated:', open_file)\n",
    "            updated_open = open(open_file, 'rb')\n",
    "            updated_load = pickle.load(updated_open)\n",
    "            memory_db.append(updated_load)\n",
    "        else:\n",
    "            memory_db.append(open_file)\n",
    "            \n",
    "    return memory_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: NCAAB_2019.pickle\n"
     ]
    }
   ],
   "source": [
    "pickled_spread = update_pickle(pickled_spreads, bet_type='spread')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: NCAAB_ML2019.pickle\n"
     ]
    }
   ],
   "source": [
    "pickled_mls = update_pickle(pickled_mls, bet_type='moneyline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    \"\"\"\n",
    "    Takes a line of HTML from SBR and parses out the spread and odds.\n",
    "    line.text in form of '+12½ -106'\n",
    "    It then returns a list containing the game_id, team_id, spread, and odds.\n",
    "    \"\"\"\n",
    "    game_id = re.findall('(\\d{5,7})', line.attrs['id'])\n",
    "    team_id = re.findall('(\\d{3,4}).[1]$', line.attrs['id'])\n",
    "    game_line = line.text\n",
    "    if len(line.text) < 1:\n",
    "        game_line = None\n",
    "    else:\n",
    "        game_line = game_line.replace(u'\\xa0', u' ')\n",
    "        if '½' in game_line:\n",
    "            game_line = game_line.replace('½', '.5')\n",
    "        if 'PK' in game_line:\n",
    "            bet_info = re.findall('([-+][\\d]*)', game_line)\n",
    "            spread = 'PK'\n",
    "            odds = bet_info[0]\n",
    "        else:            \n",
    "            bet_info = re.findall('([^\\s]+)', game_line)\n",
    "            spread = bet_info[0]\n",
    "            try:\n",
    "                odds = bet_info[1]\n",
    "                #odds = re.findall('\\s(.*)', game_line)\n",
    "            except:\n",
    "                odds = None\n",
    "        return [game_id[0], team_id[0], spread, odds]    \n",
    "    return [game_id[0], team_id[0], game_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_grabber(soup):\n",
    "    \"\"\"\n",
    "    Takes a SBR soup object as input and returns a tuple of bs4 \n",
    "    class tags sorted by Sportsbook.\n",
    "    \"\"\"\n",
    "    pinnacle, fiveDimes, betOnline, bookMaker, opener = {'book' : 'Pinnacle'}, {'book' : 'FiveDimes'}, {'book' : 'BetOnline'},\\\n",
    "                                                        {'book' : 'BookMaker'}, {'book' : 'Openers'}\n",
    "    bookline = soup.find_all(class_='eventLine-book-value')\n",
    "    for line in bookline:\n",
    "        if line.has_attr('id'):\n",
    "            game_data = parse_line(line)\n",
    "            game_id = game_data.pop(0)\n",
    "            if 'eventLineOpener' in str(line) and game_id not in opener:           \n",
    "                opener[game_id] = [game_data] \n",
    "            elif 'eventLineOpener' in str(line) and game_id in opener:\n",
    "                opener[game_id].append(game_data)\n",
    "            elif pinnacle_id in str(line) and game_id not in pinnacle:\n",
    "                pinnacle[game_id] = [game_data]\n",
    "            elif pinnacle_id in str(line) and game_id in pinnacle:\n",
    "                pinnacle[game_id].append(game_data)\n",
    "            elif fiveDimes_id in str(line) and game_id not in fiveDimes:\n",
    "                fiveDimes[game_id] = [game_data]\n",
    "            elif fiveDimes_id in str(line) and game_id in fiveDimes:\n",
    "                fiveDimes[game_id].append(game_data)\n",
    "            elif bookMaker_id in str(line) and game_id not in bookMaker:\n",
    "                bookMaker[game_id] = [game_data]\n",
    "            elif bookMaker_id in str(line) and game_id in bookMaker:\n",
    "                bookMaker[game_id].append(game_data)\n",
    "            elif betOnline_id in str(line) and game_id not in betOnline:\n",
    "                betOnline[game_id] = [game_data]\n",
    "            elif betOnline_id in str(line) and game_id in betOnline:\n",
    "                betOnline[game_id].append(game_data)\n",
    "\n",
    "    return (opener, pinnacle, fiveDimes, bookMaker, betOnline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_line_grabber(ml_soup):\n",
    "    pinnacle, fiveDimes, betOnline, bookMaker, opener = {'book' : 'Pinnacle'}, {'book' : 'FiveDimes'}, {'book' : 'BetOnline'},\\\n",
    "                                                        {'book' : 'BookMaker'}, {'book' : 'Openers'}\n",
    "    data = []\n",
    "    bookline = ml_soup.find_all(class_='eventLine-book-value')\n",
    "    for line in bookline:\n",
    "        try:\n",
    "            info_string = line.attrs['id']            \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        odds_line = re.findall('([-|+][\\d]*)', line.text)\n",
    "#            if bet_type == 'spread':            \n",
    "#                spread = odds_line[0]\n",
    "#                odds = odds_line[1]\n",
    "#            elif bet_type == 'moneyline':\n",
    "        if len(odds_line) > 0:\n",
    "            ml_odds = odds_line[0]\n",
    "        else:\n",
    "            ml_odds = None\n",
    "        if '999996' in info_string or'169' in info_string or'180' in info_string or '139' in info_string or '1275' in info_string or '999991' in info_string:\n",
    "            continue\n",
    "        if pinnacle_id in info_string or bookMaker_id in info_string or fiveDimes_id in info_string or betOnline_id in info_string:\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "        game_info= re.findall('-(\\d*)', info_string)\n",
    "        game_id = game_info[0]\n",
    "        sb_id = game_info[1]\n",
    "        if 'eventLineOpener' in str(line):\n",
    "            opener = True\n",
    "        else:\n",
    "            opener = False\n",
    "        team_id = game_info[2]\n",
    "        data.append([game_id, team_id, sb_id, ml_odds, opener])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxscore(soup, game_id):\n",
    "    \"\"\"\n",
    "    Scrapes the boxscore for the games.\n",
    "    Returns a list with 2 list, home and away.\n",
    "    \"\"\"\n",
    "    scores = soup.find(class_='score', id='score-' + game_id)\n",
    "    contents = scores.contents\n",
    "    \n",
    "    home = contents[0]    \n",
    "    periods_home = home.findAll(class_='period')\n",
    "    final_home_first = (home.findAll(class_='first total'))\n",
    "    final_home = (home.findAll(class_='total'))[0]\n",
    "    \n",
    "    boxscore_home = [periods_home[game].text for game in range(0, len(periods_home))]\n",
    "    \n",
    "    if len(final_home_first) >= 1:\n",
    "        final_home_first = final_home_first[0].text\n",
    "        boxscore_home.append(final_home_first)\n",
    "    elif len(final_home) >= 1:\n",
    "        final_home = final_home[0].text\n",
    "        boxscore_home.append(final_home)\n",
    "    else:\n",
    "        boxscore_home.append(np.nan)\n",
    "    for idx in range(0, len(boxscore_home)):\n",
    "        if boxscore_home[idx] == '':\n",
    "            boxscore_home[idx] = np.nan\n",
    "        elif boxscore_home[idx] == u'\\xa0':\n",
    "            boxscore_home[idx] = None\n",
    "\n",
    "    away = contents[1]\n",
    "    periods_away = away.findAll(class_='period')\n",
    "    final_away_first = (away.findAll(class_='first total'))\n",
    "    final_away = (away.findAll(class_='total'))\n",
    "    \n",
    "    boxscore_away = [periods_away[game].text for game in range(0, len(periods_away))]\n",
    "    if len(final_away_first) >= 1:\n",
    "        final_away_first = final_away_first[0].text\n",
    "        boxscore_away.append(final_away_first)\n",
    "    elif len(final_away) >= 1:\n",
    "        final_away = final_away[0].text\n",
    "        boxscore_away.append(final_away)\n",
    "    else:\n",
    "        boxscore_away.append(np.nan)\n",
    "    for idx in range(0, len(boxscore_away)):\n",
    "        if boxscore_away[idx] == '':\n",
    "            boxscore_away[idx] = np.nan\n",
    "        elif boxscore_away[idx] == u'\\xa0':\n",
    "            boxscore_away[idx] = None\n",
    "    \n",
    "    return [boxscore_home, boxscore_away]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_data(soup):\n",
    "    \"\"\"\n",
    "    Takes a URL address from SBR, creates a BeautifulSoup object and then proceeds\n",
    "    to parse the JSON data from within.  \n",
    "    The data is returned in the form of a dictionary of list, whose keys are the \n",
    "    game ids used by the website.  The dict values are list for each game that\n",
    "    also contain 3 list, home, away, general info.\n",
    "    \"\"\"\n",
    "    js = soup.select(\"[type='application/ld+json']\")\n",
    "    \n",
    "    data = {}\n",
    "    for game in js:\n",
    "        game_text = json.loads(game.text)        \n",
    "        if game_text['name'] == \" vs \":\n",
    "            continue\n",
    "        else:\n",
    "            team_ids = game_text['name']\n",
    "            if '  ' in team_ids:\n",
    "                team_ids = team_ids.replace('  ', ' & ')\n",
    "            if 'AM' in team_ids:\n",
    "                team_ids = team_ids.replace('AM', 'A&M')\n",
    "            home_id = (re.findall('.+?(?= vs)', team_ids))[0]\n",
    "            away_id = (re.findall('(?=vs).*', team_ids))[0][3:]\n",
    "           \n",
    "        game_id = game_text['@id']\n",
    "        url = game_text['url']\n",
    "        date_time = game_text['startDate']\n",
    "        date = date_time[:10]\n",
    "        start_time = date_time[11:]\n",
    "        \n",
    "        arena = game_text['location']['name']\n",
    "        if 'addressLocality' in game_text['location']['address']:\n",
    "            venue_city = game_text['location']['address']['addressLocality']\n",
    "        else:\n",
    "            venue_city = np.nan\n",
    "        if 'addressRegion' in game_text['location']['address']:\n",
    "            venue_state = game_text['location']['address']['addressRegion']\n",
    "        else:\n",
    "            venue_state = np.nan\n",
    "            \n",
    "        home = {'name' : home_id}\n",
    "        away = {'name' : away_id}\n",
    "        info = {'game_id' : game_id, 'teams' : team_ids, 'url' : url, 'arena' : arena, 'city' : venue_city, \n",
    "                'state' : venue_state, 'date' : date, 'time' : start_time, 'date_time' : date_time}\n",
    "        data[game_id] = {'home':home, 'away': away, 'info': info}\n",
    "    \n",
    "    for game_id, info_dicts in data.items():\n",
    "        boxscores = boxscore(soup, game_id)\n",
    "        info_dicts['home']['home_score'] = boxscores[0]\n",
    "        info_dicts['away']['away_score'] = boxscores[1]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_id_name(soup):\n",
    "    \"\"\"\n",
    "    Retrieves and matches Team ID numbers with Team Names.\n",
    "    Returns a dictionary with ID# as key, team name as value.\n",
    "    \"\"\"\n",
    "    id_name_list = {}\n",
    "    name_tag = soup.findAll(class_='team-name')\n",
    "    for data in name_tag:\n",
    "        a_tag = data.findAll('a', href=True)[0]\n",
    "        href = a_tag['href']\n",
    "        team_name = data.text\n",
    "        team_name = team_name.replace(u'\\xa0', u'')\n",
    "        try:\n",
    "            if '(' in team_name or ')' in team_name:\n",
    "                try:\n",
    "                    rank = re.findall('^[(]([\\d]*)[)]', team_name)[0]\n",
    "                    team_name = re.findall('([A-Z].*[a-z]*)', team_name)[0]\n",
    "                    if '  ' in team_name:\n",
    "                        print('found match')\n",
    "                except:\n",
    "                    rank = None\n",
    "            else:\n",
    "                rank = None\n",
    "            team_id = data.attrs['rel']\n",
    "            id_name_list[team_name] = [team_id, href]\n",
    "        except:\n",
    "            print (team_name)\n",
    "            continue\n",
    "        if rank != None and len(rank) > 0:\n",
    "            id_name_list[team_name].append(rank)\n",
    "        else:\n",
    "            id_name_list[team_name].append(None)\n",
    "             \n",
    "    return id_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_info_json(oJson, name_to_id):    \n",
    "    for game, info in oJson.items():\n",
    "        url = info['info']['url']\n",
    "        mascot_home = info['home']['name']\n",
    "        mascot_away = info['away']['name']\n",
    "        for team, team_id in name_to_id.items():\n",
    "            if team_id[1] == url:\n",
    "                if team in mascot_home:            \n",
    "                    info['home']['home_name'] = team\n",
    "                    info['home']['home_id'] = team_id[0]\n",
    "                    info['home']['rank'] = team_id[2]\n",
    "                if team in mascot_away:\n",
    "                    info['away']['away_name'] = team\n",
    "                    info['away']['away_id'] = team_id[0]\n",
    "                    info['away']['rank'] = team_id[2] \n",
    "    return oJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_spread_combiner(money_line_grabber_result, line_grabber_dict, json_dict):\n",
    "    \"\"\"\n",
    "    Uses the helper dictionary sb_key in order to combine the money lines for games with spreads.\n",
    "    \"\"\"\n",
    "\n",
    "    #print(line_grabber_dict)\n",
    "    for date, games in money_line_grabber_result.items():\n",
    "        for game in games:\n",
    "            sb_key = {'openers' : ['opener', line_grabber_dict[date][0]],\n",
    "                      pinnacle_id : ['Pinnacle', line_grabber_dict[date][1]],\n",
    "                      fiveDimes_id : ['Five Dimes', line_grabber_dict[date][2]],\n",
    "                      bookMaker_id : ['BookMaker', line_grabber_dict[date][3]],\n",
    "                      betOnline_id : ['BetOnline', line_grabber_dict[date][4]]}\n",
    "            game_id = game[0]\n",
    "            sbid = game[2]\n",
    "            team_id = game[1]\n",
    "            ml_odds = game[3]\n",
    "            _open = game[4]\n",
    "            if _open == True:\n",
    "                sbid = 'openers'\n",
    "            json_game = json_dict[date][game_id]\n",
    "                       \n",
    "            for book, data in sb_key.items():    \n",
    "                try:\n",
    "                    spreads = data[1][game_id]\n",
    "                except: \n",
    "                    bookie = data[0]\n",
    "                    if team_id == json_game['home']['home_id']:\n",
    "                        json_dict[date][game_id]['home']['spread'] = {bookie : None}\n",
    "                    elif team_id == json_game['away']['away_id']:\n",
    "                        json_dict[date][game_id]['away']['spread'] = {bookie : None}\n",
    "                    continue\n",
    "                    \n",
    "                bookie = data[0]\n",
    "                for spread in spreads:\n",
    "                    if team_id == spread[0]:\n",
    "                        team_spread = spread[1:]\n",
    "                        if team_id == json_game['home']['home_id']:\n",
    "                            if 'money_line' not in json_dict[date][game_id]['home']:\n",
    "                                json_dict[date][game_id]['home']['money_line'] = {sb_key[sbid][0] : ml_odds}\n",
    "                            else:\n",
    "                                json_dict[date][game_id]['home']['money_line'].update({sb_key[sbid][0] : ml_odds})\n",
    "                            if 'spread' not in json_dict[date][game_id]['home']:\n",
    "                                json_dict[date][game_id]['home']['spread'] = {bookie : team_spread}\n",
    "                            else:\n",
    "                                json_dict[date][game_id]['home']['spread'].update({bookie : team_spread})\n",
    "                        elif team_id == json_game['away']['away_id']:\n",
    "                            if 'money_line' not in json_dict[date][game_id]['away']:\n",
    "                                json_dict[date][game_id]['away']['money_line'] = {sb_key[sbid][0] : ml_odds}\n",
    "                            else:\n",
    "                                json_dict[date][game_id]['away']['money_line'].update({sb_key[sbid][0] : ml_odds})\n",
    "                            if 'spread' not in json_dict[date][game_id]['away']:\n",
    "                                json_dict[date][game_id]['away']['spread'] = {bookie : team_spread}\n",
    "                            else:\n",
    "                                json_dict[date][game_id]['away']['spread'].update({bookie : team_spread})\n",
    "    return json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    " def clean_soup(spread_page_range, ml_page_range):\n",
    "    \"\"\"\n",
    "    Takes as input a dictionary with key values of dates ('YYYYMMDD') and\n",
    "    values of BeautifulSoup data.\n",
    "    \"\"\"\n",
    "    spread_data = {}\n",
    "    json_dict = {}\n",
    "    for date, page in spread_page_range.items():\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        oJson = json_data(soup) # oJson[game_id][home/away/info][...]\n",
    "        name_to_id = match_id_name(soup) # team_name : team_id\n",
    "        updated_ojson = pair_info_json(oJson, name_to_id)        \n",
    "        game_spreads = line_grabber(soup)\n",
    "        spread_data[date] = game_spreads\n",
    "        json_dict[date] = updated_ojson\n",
    "\n",
    "    ml_data = {}\n",
    "    for date, page in ml_page_range.items():\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        money_lines = ml_line_grabber(soup)\n",
    "        ml_data[date] = money_lines       \n",
    "    \n",
    "    results = ml_spread_combiner(ml_data, spread_data, json_dict) \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = {'20141114': requests.get(SPREAD_URL+str(cbb14[0]))}\n",
    "mlpage = {'20141114' : requests.get(ML_URL + str(cbb14[0]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "mlsoup = BeautifulSoup(mlpage.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_spreads = line_grabber(soup)\n",
    "money_lines = ml_line_grabber(mlsoup)\n",
    "#(opener, pinnacle, fiveDimes, bookMaker, betOnline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = clean_soup(page, mlpage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
